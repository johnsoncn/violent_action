{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide explains how to train your own **custom dataset** with YOLOv5.\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "Clone this repo, download tutorial dataset, and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies, including **Python>=3.7** and **PyTorch>=1.5**.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ultralytics/yolov5 # clone repo\n",
    "python3 -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1n_oKgR81BJtqk75b00eAjdv03qVCQn2f','coco128.zip')\" # download dataset\n",
    "cd yolov5\n",
    "pip install -U -r requirements.txt\n",
    "```\n",
    "## Fixes for issues with Windows install\n",
    "Having issues with installing under Windows? Try the following.\n",
    "```bash\n",
    "pip install torch==1.5.1+cpu torchvision==0.6.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "pip install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"\n",
    "pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
    "```\n",
    "## Train On Custom Data\n",
    "\n",
    "### 1. Create Dataset.yaml\n",
    "\n",
    "[data/coco128.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml) is a small tutorial dataset composed of the first 128 images in [COCO](http://cocodataset.org/#home) train2017 (go to tutorial_yolov5). These same 128 images are used for both training and validation in this example. `coco128.yaml` defines 1) a path to a directory of training images (or path to a *.txt file with a list of training images), 2) the same for our validation images, 3) the number of classes, 4) a list of class names:\n",
    "```yaml\n",
    "# train and val datasets (image directory or *.txt file with image paths)\n",
    "train: ../coco128/images/train2017/\n",
    "val: ../coco128/images/val2017/\n",
    "\n",
    "# number of classes\n",
    "nc: 80\n",
    "\n",
    "# class names\n",
    "names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "        'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', \n",
    "        'teddy bear', 'hair drier', 'toothbrush']\n",
    "```\n",
    "\n",
    "\n",
    "### 2. Create Labels\n",
    "\n",
    "After using a tool like [Labelbox](https://labelbox.com/) or [CVAT](https://github.com/opencv/cvat) to label your images, export your labels to **darknet format**, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The `*.txt` file specifications are:\n",
    "\n",
    "- One row per object\n",
    "- Each row is `class x_center y_center width height` format.\n",
    "- Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels, divide `x_center` and `width` by image width, and `y_center` and `height` by image height.\n",
    "- Class numbers are zero-indexed (start from 0).\n",
    "\n",
    "Each image's label file should be locatable by simply replacing `/images/*.jpg` with `/labels/*.txt` in its pathname. An example image and label pair would be:\n",
    "```bash\n",
    "dataset/images/train2017/000000109622.jpg  # image\n",
    "dataset/labels/train2017/000000109622.txt  # label\n",
    "```\n",
    "An example label file with 5 persons (all class `0`):  \n",
    "<img width=\"475\" alt=\"Screen Shot 2020-04-01 at 11 44 26 AM\" src=\"https://user-images.githubusercontent.com/26833433/78174482-307bb800-740e-11ea-8b09-840693671042.png\">\n",
    "\n",
    "\n",
    "### 3. Organize Directories\n",
    "\n",
    "Organize your train and val images and labels according to the example below. Note `/coco128` should be **next to** the `/yolov5` directory. Make sure `coco128/labels` folder is next to `coco128/images` folder.\n",
    "\n",
    "<img width=\"750\" alt=\"Screen Shot 2020-04-01 at 11 44 26 AM\" src=\"https://user-images.githubusercontent.com/26833433/83666389-bab4d980-a581-11ea-898b-b25471d37b83.jpg\">\n",
    "\n",
    "### 4. Select a Model\n",
    "\n",
    "Select a model from the `./models` folder. Here we select [yolov5s.yaml](https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml), the smallest and fastest model available. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models. Once you have selected a model, if you are not training COCO, **update** the `nc: 80` parameter in your yaml file to match the number of classes in your dataset from step **1.**\n",
    "```yaml\n",
    "# parameters\n",
    "nc: 80  # number of classes   <------------------  UPDATE to match your dataset\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "\n",
    "# anchors\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Focus, [64, 3]],  # 1-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 2-P2/4\n",
    "   [-1, 3, Bottleneck, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 4-P3/8\n",
    "   [-1, 9, BottleneckCSP, [256, False]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 6-P4/16\n",
    "   [-1, 9, BottleneckCSP, [512, False]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]], # 8-P5/32\n",
    "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
    "   [-1, 12, BottleneckCSP, [1024, False]],  # 10\n",
    "  ]\n",
    "\n",
    "# YOLOv5 head\n",
    "head:\n",
    "  [[-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],  # 12 (P5/32-large)\n",
    "\n",
    "   [-2, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 3, BottleneckCSP, [512, False]],\n",
    "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],  # 16 (P4/16-medium)\n",
    "\n",
    "   [-2, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 3, BottleneckCSP, [256, False]],\n",
    "   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],  # 21 (P3/8-small)\n",
    "\n",
    "   [[], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n",
    "```\n",
    "\n",
    "### 5. Train\n",
    "\n",
    "Run the training command below to train `coco128.yaml` for 5 epochs. You can train YOLOv5s from scratch by passing `--cfg yolov5s.yaml --weights ''`, or train from a pretrained checkpoint by passing a matching weights file: `--cfg yolov5s.yaml --weights yolov5s.pt`.\n",
    "```bash\n",
    "# Train YOLOv5s on coco128 for 5 epochs\n",
    "$ python train.py --img 640 --batch 16 --epochs 5 --data ./data/coco128.yaml --cfg ./models/yolov5s.yaml --weights ''\n",
    "```\n",
    "\n",
    "For training command outputs and further details please see the training section of Google Colab Notebook. <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "\n",
    "### 6. Visualize\n",
    "\n",
    "After training starts, view `train*.jpg` images to see training images, labels and augmentation effects. Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934). If your labels are not correct in these images then you have incorrectly labelled your data, and should revisit **2. Create Labels**.\n",
    "![download](https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg)\n",
    "\n",
    "After the first epoch is complete, view `test_batch0_gt.jpg` to see test batch 0 ground truth labels:\n",
    "![download (1)](https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg)\n",
    "\n",
    "And view `test_batch0_pred.jpg` to see test batch 0 predictions:\n",
    "![download (2)](https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg)\n",
    "\n",
    "Training losses and performance metrics are saved to Tensorboard and also to a `results.txt` logfile. `results.txt` is plotted as `results.png` after training completes. Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`. Here we show YOLOv5s trained on coco128 to 100 epochs, starting from scratch (orange), and starting from pretrained `yolov5s.pt` weights (blue):\n",
    "\n",
    "![download](https://user-images.githubusercontent.com/26833433/83667810-d7eaa780-a583-11ea-8de8-5cca0673d076.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
