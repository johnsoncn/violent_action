{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1d425eea-d44e-434a-991c-01eb15abaab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0 True\n",
      "0.15.0\n",
      "11.3\n",
      "MSVC 192930037\n",
      "OS: Windows-10-10.0.22000-SP0\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())\n",
    "\n",
    "# check os platform\n",
    "import platform\n",
    "print('OS: {}'.format(platform.platform()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG MMACTION2\n",
    "1. Option (RECOMMENDED): create a customconfig.py file \n",
    "2. Option: modify other config and train from script (see folder MMACTION2/TESTS/mmaction2_tutorial)\n",
    "\n",
    "#NOTE: VideoDataset working; RawframeDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmaction2 dir\n",
    "mmadir=\"C:/Users/nmc_costa/Desktop/Desk/mmaction2/\" \n",
    "work_dir_root= mmadir\n",
    "if str(platform.platform()).upper().find('linux'.upper())>-1: \n",
    "    mmadir=\"/home/administrator/Z/Algorithms/mmaction2/\"\n",
    "    work_dir_root= '/home/administrator/Z/Work/EASYRIDE/P19/NC/mmaction2/TESTS/train/' #defualt: './'\n",
    "\n",
    "\n",
    "from mmcv import Config\n",
    "\n",
    "root= mmadir+\"data/\" #default: \"data/\"\n",
    "dataset=\"kinetics400_tiny_v2\"\n",
    "dataset_type = 'VideoDataset'\n",
    "if dataset_type=='RawframeDataset': \n",
    "    dtype_name=\"rawframes\"\n",
    "    cfg = Config.fromfile(mmadir+'/configs/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb.py')\n",
    "if dataset_type=='VideoDataset': \n",
    "    dtype_name=\"videos\"\n",
    "    cfg = Config.fromfile(mmadir+'/configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False),\n",
      "    cls_head=dict(\n",
      "        type='TSNHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.4,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips=None))\n",
      "optimizer = dict(type='SGD', lr=7.8125e-05, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(policy='step', step=[40, 80])\n",
      "total_epochs = 30\n",
      "checkpoint_config = dict(interval=10)\n",
      "log_config = dict(interval=5, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmaction/mmaction-v1/recognition/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/videos_train'\n",
      "data_root_val = 'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/videos_val'\n",
      "ann_file_train = 'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/kinetics400_tiny_v2_train_videos.txt'\n",
      "ann_file_val = 'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/kinetics400_tiny_v2_val_videos.txt'\n",
      "ann_file_test = 'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/kinetics400_tiny_v2_val_videos.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='ThreeCrop', crop_size=256),\n",
      "    dict(type='Flip', flip_ratio=0),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=2,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file=\n",
      "        'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/kinetics400_tiny_v2_train_videos.txt',\n",
      "        data_prefix=\n",
      "        'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/videos_train',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file=\n",
      "        'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/kinetics400_tiny_v2_val_videos.txt',\n",
      "        data_prefix=\n",
      "        'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/videos_val',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file=\n",
      "        'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/kinetics400_tiny_v2_val_videos.txt',\n",
      "        data_prefix=\n",
      "        'C:/Users/nmc_costa/Desktop/Desk/mmaction2/data/kinetics400_tiny_v2/videos_val',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='ThreeCrop', crop_size=256),\n",
      "            dict(type='Flip', flip_ratio=0),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=5, metrics=['top_k_accuracy', 'mean_class_accuracy'])\n",
      "work_dir = 'C:/Users/nmc_costa/Desktop/Desk/mmaction2/work_dirs/tsn_kinetics_pretrained_r50_1x1x3_50e_k400tiny_rgb/'\n",
      "omnisource = False\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modify dataset type and path\n",
    "cfg.dataset_type = dataset_type\n",
    "cfg.data_root =  root+dataset+'/'+dtype_name+'_train'\n",
    "cfg.data_root_val = root+dataset+'/'+dtype_name+'_val'\n",
    "cfg.ann_file_train =  root+dataset+'/'+dataset+'_train_'+dtype_name+'.txt'\n",
    "cfg.ann_file_val = root+dataset+'/'+dataset+'_val_'+dtype_name+'.txt'\n",
    "cfg.ann_file_test = root+dataset+'/'+dataset+'_val_'+dtype_name+'.txt'\n",
    "\n",
    "cfg.data.test.type = dataset_type\n",
    "cfg.data.test.ann_file = root+dataset+'/'+dataset+'_val_'+dtype_name+'.txt'\n",
    "cfg.data.test.data_prefix = cfg.data_root_val \n",
    "\n",
    "cfg.data.train.type = dataset_type\n",
    "cfg.data.train.ann_file = root+dataset+'/'+dataset+'_train_'+dtype_name+'.txt'\n",
    "cfg.data.train.data_prefix = cfg.data_root\n",
    "\n",
    "cfg.data.val.type = dataset_type\n",
    "cfg.data.val.ann_file = root+dataset+'/'+dataset+'_val_'+dtype_name+'.txt'\n",
    "cfg.data.val.data_prefix = cfg.data_root_val \n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmaction/mmaction-v1/recognition/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = work_dir_root+'work_dirs/tsn_kinetics_pretrained_r50_1x1x3_50e_k400tiny_rgb/'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "# we also only use 2 videos per gpu\n",
    "cfg.data.videos_per_gpu = cfg.data.videos_per_gpu // 16\n",
    "cfg.optimizer.lr = cfg.optimizer.lr / 8 / 16\n",
    "cfg.total_epochs = 30\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 10\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 5\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "from mmcv.runner import set_random_seed\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tES-qnZ3k38Z"
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "85a52ef3-7b5c-4c52-8fef-00322a8c65e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_torchvision loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:19:47,945 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.weight', 'fc.bias'}\n",
      "2021-09-20 16:19:51,620 - mmaction - INFO - load checkpoint from https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmaction/mmaction-v1/recognition/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
      "2021-09-20 16:19:51,621 - mmaction - INFO - Use load_from_http loader\n",
      "2021-09-20 16:19:51,764 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2021-09-20 16:19:51,772 - mmaction - INFO - Start running, host: nmc_costa@DESKTOP-C3GGF8U, work_dir: C:\\Users\\nmc_costa\\Desktop\\Desk\\mmaction2\\work_dirs\\tsn_kinetics_pretrained_r50_1x1x3_50e_k400tiny_rgb\n",
      "2021-09-20 16:19:51,772 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2021-09-20 16:19:51,773 - mmaction - INFO - workflow: [('train', 1)], max: 30 epochs\n",
      "C:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\hooks\\evaluation.py:192: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n",
      "2021-09-20 16:20:02,060 - mmaction - INFO - Epoch [1][5/15]\tlr: 7.813e-05, eta: 0:15:14, time: 2.054, data_time: 1.418, memory: 1649, top1_acc: 0.4000, top5_acc: 1.0000, loss_cls: 0.7604, loss: 0.7604, grad_norm: 14.8813\n",
      "2021-09-20 16:20:03,037 - mmaction - INFO - Epoch [1][10/15]\tlr: 7.813e-05, eta: 0:08:14, time: 0.196, data_time: 0.002, memory: 1649, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.6282, loss: 0.6282, grad_norm: 10.1833\n",
      "2021-09-20 16:20:04,021 - mmaction - INFO - Epoch [1][15/15]\tlr: 7.813e-05, eta: 0:05:54, time: 0.196, data_time: 0.002, memory: 1649, top1_acc: 0.4000, top5_acc: 1.0000, loss_cls: 0.7165, loss: 0.7165, grad_norm: 10.8553\n",
      "2021-09-20 16:20:12,645 - mmaction - INFO - Epoch [2][5/15]\tlr: 7.813e-05, eta: 0:07:21, time: 1.658, data_time: 1.446, memory: 1649, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.6444, loss: 0.6444, grad_norm: 11.3932\n",
      "2021-09-20 16:20:13,637 - mmaction - INFO - Epoch [2][10/15]\tlr: 7.813e-05, eta: 0:06:05, time: 0.198, data_time: 0.003, memory: 1649, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.7155, loss: 0.7155, grad_norm: 12.3879\n",
      "2021-09-20 16:20:14,636 - mmaction - INFO - Epoch [2][15/15]\tlr: 7.813e-05, eta: 0:05:15, time: 0.200, data_time: 0.002, memory: 1649, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6797, loss: 0.6797, grad_norm: 10.9274\n",
      "2021-09-20 16:20:23,068 - mmaction - INFO - Epoch [3][5/15]\tlr: 7.813e-05, eta: 0:06:03, time: 1.621, data_time: 1.413, memory: 1649, top1_acc: 0.4000, top5_acc: 1.0000, loss_cls: 0.7126, loss: 0.7126, grad_norm: 11.1642\n",
      "2021-09-20 16:20:24,050 - mmaction - INFO - Epoch [3][10/15]\tlr: 7.813e-05, eta: 0:05:23, time: 0.196, data_time: 0.002, memory: 1649, top1_acc: 0.4000, top5_acc: 1.0000, loss_cls: 0.6635, loss: 0.6635, grad_norm: 12.1192\n",
      "2021-09-20 16:20:25,035 - mmaction - INFO - Epoch [3][15/15]\tlr: 7.813e-05, eta: 0:04:53, time: 0.197, data_time: 0.002, memory: 1649, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6978, loss: 0.6978, grad_norm: 10.3155\n",
      "2021-09-20 16:20:33,323 - mmaction - INFO - Epoch [4][5/15]\tlr: 7.813e-05, eta: 0:05:24, time: 1.597, data_time: 1.394, memory: 1649, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6795, loss: 0.6795, grad_norm: 12.0900\n",
      "2021-09-20 16:20:34,315 - mmaction - INFO - Epoch [4][10/15]\tlr: 7.813e-05, eta: 0:04:58, time: 0.198, data_time: 0.003, memory: 1649, top1_acc: 0.4000, top5_acc: 1.0000, loss_cls: 0.7414, loss: 0.7414, grad_norm: 12.6040\n",
      "2021-09-20 16:20:35,292 - mmaction - INFO - Epoch [4][15/15]\tlr: 7.813e-05, eta: 0:04:36, time: 0.196, data_time: 0.002, memory: 1649, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6761, loss: 0.6761, grad_norm: 11.2122\n",
      "2021-09-20 16:20:43,534 - mmaction - INFO - Epoch [5][5/15]\tlr: 7.813e-05, eta: 0:04:59, time: 1.592, data_time: 1.390, memory: 1649, top1_acc: 0.4000, top5_acc: 1.0000, loss_cls: 0.6899, loss: 0.6899, grad_norm: 12.3522\n",
      "2021-09-20 16:20:44,522 - mmaction - INFO - Epoch [5][10/15]\tlr: 7.813e-05, eta: 0:04:39, time: 0.197, data_time: 0.002, memory: 1649, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.6337, loss: 0.6337, grad_norm: 12.3516\n",
      "2021-09-20 16:20:45,505 - mmaction - INFO - Epoch [5][15/15]\tlr: 7.813e-05, eta: 0:04:22, time: 0.197, data_time: 0.002, memory: 1649, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.6425, loss: 0.6425, grad_norm: 9.7316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 1.7 task/s, elapsed: 6s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-20 16:20:51,939 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2021-09-20 16:20:51,940 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2021-09-20 16:20:51,941 - mmaction - INFO - Evaluating mean_class_accuracy ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "y_real dtype must be np.int64, but got int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-75094123c5f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Create work_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmmcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir_or_exist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\apis\\train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, dataset, cfg, distributed, validate, test, timestamp, meta)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0momnisource\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mrunner_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mrunner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_last'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_best'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_epochs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                     \u001b[0mepoch_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# wait for some hooks like loggers to finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_train_epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\base_runner.py\u001b[0m in \u001b[0;36mcall_hook\u001b[1;34m(self, fn_name)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_hook_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\hooks\\evaluation.py\u001b[0m in \u001b[0;36mafter_train_epoch\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;34m\"\"\"Called after every training epoch to evaluate the results.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby_epoch\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_do_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\hooks\\evaluation.py\u001b[0m in \u001b[0;36m_do_evaluate\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eval_iter_num'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0mkey_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_best\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_ckpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Tools\\miniconda3\\envs\\violent_action\\lib\\site-packages\\mmcv\\runner\\hooks\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, runner, results)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         eval_res = self.dataloader.dataset.evaluate(\n\u001b[0m\u001b[0;32m    325\u001b[0m             results, logger=runner.logger, **self.eval_kwargs)\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\datasets\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, results, metrics, metric_options, logger, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean_class_accuracy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                 \u001b[0mmean_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_class_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 \u001b[0meval_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_class_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mlog_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'\\nmean_acc\\t{mean_acc:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\core\\evaluation\\accuracy.py\u001b[0m in \u001b[0;36mmean_class_accuracy\u001b[1;34m(scores, labels)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \"\"\"\n\u001b[0;32m     78\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mcf_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mcls_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcf_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\core\\evaluation\\accuracy.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_pred, y_real, normalize)\u001b[0m\n\u001b[0;32m     35\u001b[0m             f'y_real must be list or np.ndarray, but got {type(y_real)}')\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my_real\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m     38\u001b[0m             f'y_real dtype must be np.int64, but got {y_real.dtype}')\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: y_real dtype must be np.int64, but got int32"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryVoSfZVmogw"
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "200e37c7-0da4-421f-98da-41418c3110ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 1.3 task/s, elapsed: 8s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "y_real dtype must be np.int64, but got int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b849d16708d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0meval_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0meval_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'interval'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0meval_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0meval_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{name}: {val:.04f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\datasets\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, results, metrics, metric_options, logger, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean_class_accuracy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                 \u001b[0mmean_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_class_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m                 \u001b[0meval_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_class_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mlog_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'\\nmean_acc\\t{mean_acc:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\core\\evaluation\\accuracy.py\u001b[0m in \u001b[0;36mmean_class_accuracy\u001b[1;34m(scores, labels)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \"\"\"\n\u001b[0;32m     78\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mcf_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mcls_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcf_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nmc_costa\\desktop\\desk\\mmaction2\\mmaction\\core\\evaluation\\accuracy.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_pred, y_real, normalize)\u001b[0m\n\u001b[0;32m     35\u001b[0m             f'y_real must be list or np.ndarray, but got {type(y_real)}')\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my_real\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m     38\u001b[0m             f'y_real dtype must be np.int64, but got {y_real.dtype}')\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: y_real dtype must be np.int64, but got int32"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
