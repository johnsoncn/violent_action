{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets\n",
    "version: 1\n",
    "\n",
    "info:\n",
    "- Merge different datasets annotations json\n",
    "\n",
    "author: nuno costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## motionLAB Annotations Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to combine multiple datasets, it is often useful to convert them into a unified data format. \n",
    "\n",
    "Objective: This script will allow you to merge the annotations into motionLab format (COCO & TAO-style annotation file) containing Image IDs in your data.json (general) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Check at the end of this script for the formats\n",
    "\n",
    "MOLA format : motionLab annotations format\n",
    "\n",
    "COCO format : https://cocodataset.org/#format-data ; https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch\n",
    "\n",
    "TAO format : https://github.com/TAO-Dataset/tao/blob/master/tao/toolkit/tao/tao.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotate_v5 import *\n",
    "import platform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: Linux-5.11.0-37-generic-x86_64-with-glibc2.10\n",
      "root dir: /mnt/Data/Datasets/External Datasets/\n"
     ]
    }
   ],
   "source": [
    "#Define root dir dependent on OS\n",
    "rdir='D:/external_datasets/' \n",
    "outdir='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/'\n",
    "#/home/user/Data/Datasets/External Datasets/MOLA/\n",
    "if str(platform.platform()).find('Linux')>-1:\n",
    "    rdir=rdir.replace('D:/external_datasets/','/mnt/Data/Datasets/External Datasets/') #jsons file\n",
    "    outdir='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/'\n",
    "print('OS: {}'.format(platform.platform()))\n",
    "print('root dir: {}'.format(rdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INIT motionLAB JSON\n",
    "- uses annotate.init_json() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON INITIATED : /mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json\n",
      "info 5\n",
      "licenses 0\n",
      "categories 0\n",
      "videos 0\n",
      "images 0\n",
      "tracks 0\n",
      "segment_info 0\n",
      "annotations 0\n",
      "datasets 3\n"
     ]
    }
   ],
   "source": [
    "molafile=outdir+'cocotaolbo.json'\n",
    "init_json(file=molafile)\n",
    "molajson =  json.load(open(molafile))\n",
    "molajson['datasets']=[{'name': 'COCO', 'id': 1}, {'name': 'TAO', 'id': 2}, {'name': 'LBO', 'id': 3}]\n",
    "molajson['info'][\"description\"]= \"MOLA = COCO 2017 + TAO + BOSH_LBO  Dataset\"\n",
    "with open(molafile, 'w') as f:\n",
    "    json.dump(molajson, f)\n",
    "for k in molajson:\n",
    "    print(k, len(molajson[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LOAD & ORGANIZE original datasets JSONs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organize original COCO & save fullcoco\n",
    "- without changing ids - necessary if you gonna mix different types of annotations\n",
    "- #NOTE the only divergent hyperparameter between instances, captions and person_keypoints is the \"annotations\"\n",
    "- #WARNING COCO captions annotations are different from instances and person_keypoints -> #SOLUTION move caption to \"images\" hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 591753/591753 [00:00<00:00, 2726939.30it/s]\n",
      "add: images: 100%|██████████| 118287/118287 [08:55<00:00, 220.83it/s]\n",
      "100%|██████████| 25014/25014 [00:00<00:00, 2501700.61it/s]\n",
      "add: images: 100%|██████████| 5000/5000 [00:00<00:00, 6661.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >> SAVING...\n",
      "JSON SAVED : /mnt/Data/Datasets/External Datasets/COCO/2017/annotations/fullcoco2017.json \n",
      "\n",
      "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])\n",
      "{'description': 'COCO 2017 Dataset', 'url': 'http://cocodataset.org', 'version': '1.0', 'year': 2017, 'contributor': 'COCO Consortium', 'date_created': '2017/09/01'}\n",
      "8\n",
      "123287\n",
      "1170251\n",
      "80\n",
      "{'license': 4, 'file_name': 'COCO/2017/images/train2017/000000471488.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000471488.jpg', 'height': 480, 'width': 640, 'date_captured': '2013-11-15 21:47:34', 'flickr_url': 'http://farm7.staticflickr.com/6233/6286490251_4bbcaf234f_z.jpg', 'id': 471488, 'caption': 'Two motorcyclists driving on a street with parked vehicles.'}\n"
     ]
    }
   ],
   "source": [
    "# merge train\n",
    "\n",
    "### 1.Captions -> #WARNING move caption annotations to images as new \"caption\" subkey ->below\n",
    "newjson = json.load(open(rdir+'COCO/2017/annotations/captions_train2017.json'))\n",
    "ann_caption=[]\n",
    "ann_imgid=[]\n",
    "for an in tqdm(newjson['annotations']):\n",
    "    ann_caption.append(an['caption'])\n",
    "    ann_imgid.append(an['image_id'])\n",
    "### 2.Instances\n",
    "newjson =  json.load(open(rdir+'COCO/2017/annotations/instances_train2017.json'))\n",
    "key='images' # add images\n",
    "root_dir='COCO/2017/images/train2017/' \n",
    "for ik, k in enumerate(tqdm(newjson[key], desc='add: {}'.format(key))):\n",
    "    newjson[key][ik]['file_name'] = root_dir + newjson[key][ik]['file_name'] # change images: file_name\n",
    "    imgid=newjson[key][ik]['id']\n",
    "    try:\n",
    "        imgidx=ann_imgid.index(imgid) #assuming one caption per imgid\n",
    "        newjson[key][ik]['caption'] = ann_caption[imgidx] # add captions\n",
    "    except:\n",
    "        newjson[key][ik]['caption'] = 'missing caption!'         \n",
    "fulljson = newjson # init fulljson\n",
    "### 3. Person Keypoints\n",
    "newjson = json.load(open(rdir+'COCO/2017/annotations/person_keypoints_train2017.json'))\n",
    "key='annotations' # add annotations\n",
    "fulljson[key] = fulljson[key] + newjson[key]        \n",
    "fulljson['categories'][0]=newjson['categories'][0] # update person category based on person_keypoints\n",
    "\n",
    "\n",
    "# merge val\n",
    "\n",
    "### 1.Captions -> #WARNING move caption annotations to images as new \"caption\" subkey ->below\n",
    "newjson = json.load(open(rdir+'COCO/2017/annotations/captions_val2017.json'))\n",
    "ann_caption=[]\n",
    "ann_imgid=[]\n",
    "for an in tqdm(newjson['annotations']):\n",
    "    ann_caption.append(an['caption'])\n",
    "    ann_imgid.append(an['image_id'])\n",
    "### 2.Instances\n",
    "newjson = json.load(open(rdir+'COCO/2017/annotations/instances_val2017.json'))\n",
    "key='images' # add images\n",
    "root_dir='COCO/2017/images/val2017/' \n",
    "for ik, k in enumerate(tqdm(newjson[key], desc='add: {}'.format(key))):\n",
    "    newjson[key][ik]['file_name'] = root_dir + newjson[key][ik]['file_name'] # change images: file_name\n",
    "    imgid=newjson[key][ik]['id']\n",
    "    try:\n",
    "        imgidx=ann_imgid.index(imgid) #assuming one caption per imgid\n",
    "        newjson[key][ik]['caption'] = ann_caption[imgidx] # add captions\n",
    "    except:\n",
    "        newjson[key][ik]['caption'] = 'missing caption!'    \n",
    "fulljson[key] = fulljson[key] + newjson[key] \n",
    "key='annotations' # add annotations\n",
    "fulljson[key] = fulljson[key] + newjson[key] \n",
    "### 3.Person Keypoints\n",
    "newjson = json.load(open(rdir+'COCO/2017/annotations/person_keypoints_val2017.json'))\n",
    "key='annotations' # add annotations\n",
    "fulljson[key] = fulljson[key] + newjson[key]   \n",
    "\n",
    "# save\n",
    "print('\\n >> SAVING...')\n",
    "jsonfile=rdir+'COCO/2017/annotations/fullcoco2017.json'\n",
    "with open(jsonfile, 'w') as f:\n",
    "    json.dump(fulljson, f)\n",
    "print(\"JSON SAVED : {} \\n\".format(jsonfile))\n",
    "\n",
    "print(fulljson.keys())\n",
    "print(fulljson['info'])\n",
    "print(len(fulljson['licenses']))\n",
    "print(len(fulljson['images']))\n",
    "print(len(fulljson['annotations']))\n",
    "print(len(fulljson['categories']))\n",
    "print(fulljson['images'][10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organize original TAO & save fulltao\n",
    "- #WARNING TAO dataset has no annotations for some categories -> #SOLVED this was on purpose (see below on the section ANNOTATIONS FORMAT)\n",
    "- #WARNING TAO dataset has no images for some categories - #SOLVE ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rename file_name: images: 100%|██████████| 18274/18274 [00:00<00:00, 1065692.16it/s]\n",
      "rename file_name: videos: 100%|██████████| 500/500 [00:00<00:00, 1088863.97it/s]\n",
      "rename file_name: images: 100%|██████████| 36375/36375 [00:00<00:00, 977603.97it/s]\n",
      "rename file_name: videos: 100%|██████████| 988/988 [00:00<00:00, 1013196.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >> SAVING...\n",
      "JSON SAVED : /home/administrator/Z/Datasets/External Datasets/TAO/TAO_DIR/annotations/fulltao.json \n",
      "\n",
      "dict_keys(['videos', 'annotations', 'tracks', 'images', 'info', 'categories', 'licenses'])\n",
      "{'year': 2020, 'version': '0.1.20200120', 'description': 'Annotations imported from Scale', 'contributor': '', 'url': '', 'date_created': '2020-01-20 15:49:53.519740'}\n",
      "1\n",
      "54649\n",
      "167751\n",
      "1230\n"
     ]
    }
   ],
   "source": [
    "TAO_ROOT=\"TAO/TAO_DIR/\"\n",
    "# merge train\n",
    "newjson =  json.load(open(rdir+TAO_ROOT+'annotations/train.json'))\n",
    "key='images' #alter paths to datasets root\n",
    "for ik, k in enumerate(tqdm(newjson[key], desc='rename file_name: {}'.format(key))):\n",
    "    root_dir=TAO_ROOT+'frames/' # change images: file_name\n",
    "    newjson[key][ik]['file_name'] = root_dir + newjson[key][ik]['file_name']\n",
    "    root_dir=TAO_ROOT+'videos/' # change images: video\n",
    "    newjson[key][ik]['video'] = root_dir + newjson[key][ik]['video']\n",
    "key='videos' #alter paths to datasets root\n",
    "for ik, k in enumerate(tqdm(newjson[key], desc='rename file_name: {}'.format(key))):\n",
    "    root_dir=TAO_ROOT+'videos/' # change images: video\n",
    "    newjson[key][ik]['name'] = root_dir + newjson[key][ik]['name']\n",
    "fulljson = newjson\n",
    "# merge val\n",
    "newjson =  json.load(open(rdir+TAO_ROOT+'annotations/validation.json'))\n",
    "key='images' #alter paths to datasets root\n",
    "for ik, k in enumerate(tqdm(newjson[key], desc='rename file_name: {}'.format(key))):\n",
    "    root_dir=TAO_ROOT+'frames/' # change images: file_name\n",
    "    newjson[key][ik]['file_name'] = root_dir + newjson[key][ik]['file_name']\n",
    "    root_dir=TAO_ROOT+'videos/' # change images: video\n",
    "    newjson[key][ik]['video'] = root_dir + newjson[key][ik]['video']\n",
    "fulljson[key] = fulljson[key] + newjson[key]\n",
    "key='videos' #alter paths to datasets root\n",
    "for ik, k in enumerate(tqdm(newjson[key], desc='rename file_name: {}'.format(key))):\n",
    "    root_dir=TAO_ROOT+'videos/' # change images: video\n",
    "    newjson[key][ik]['name'] = root_dir + newjson[key][ik]['name']\n",
    "fulljson[key] = fulljson[key] + newjson[key] \n",
    "key='tracks'\n",
    "fulljson[key] = fulljson[key] + newjson[key] \n",
    "key='annotations'\n",
    "fulljson[key] = fulljson[key] + newjson[key] \n",
    "\n",
    "# save\n",
    "print('\\n >> SAVING...')\n",
    "jsonfile=rdir+TAO_ROOT+'annotations/fulltao.json'\n",
    "with open(jsonfile, 'w') as f:\n",
    "    json.dump(fulljson, f)\n",
    "print(\"JSON SAVED : {} \\n\".format(jsonfile))\n",
    "\n",
    "print(fulljson.keys())\n",
    "print(fulljson['info'])\n",
    "print(len(fulljson['licenses']))\n",
    "print(len(fulljson['images']))\n",
    "print(len(fulljson['annotations']))\n",
    "print(len(fulljson['categories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LBO (Bosch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organize original LBO\n",
    "- Use bosh2json.ipynb to get full LBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MERGE datasets\n",
    "- #WARNING merge is slow -> #TODO #SOLUTION use same approach as fixclasses and mixclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "molafile=molafile\n",
    "mergecoco=outdir+'fullcoco2017.json'\n",
    "mergetao=outdir+'fulltao.json'\n",
    "mergelbo=outdir+ 'LBO.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>Namespace(dataset_id=1, dir_key=['file_name', 'video'], initjson=False, mergefile='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/fullcoco2017.json', molafile='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json', root_dir=None)\n",
      "get keys info: 100%|██████████████████████████| 6/6 [00:00<00:00, 204600.20it/s]\n",
      "get keys licenses: 100%|██████████████████████| 8/8 [00:00<00:00, 279620.27it/s]\n",
      "get keys images: 100%|██████████████| 123287/123287 [00:00<00:00, 686848.94it/s]\n",
      "get keys annotations: 100%|███████| 1170251/1170251 [00:01<00:00, 700741.27it/s]\n",
      "get keys categories: 100%|██████████████████| 80/80 [00:00<00:00, 661823.12it/s]\n",
      "\n",
      " >> MERGING LICENSES...\n",
      "update key: licenses: 100%|███████████████████| 8/8 [00:00<00:00, 404270.27it/s]\n",
      "update images : license: 100%|█████| 123287/123287 [00:00<00:00, 1830064.97it/s]\n",
      "\n",
      " >> MERGING CATEGORIES...\n",
      "update key: categories: 100%|██████████████| 80/80 [00:00<00:00, 1157049.38it/s]\n",
      "update annotations : category_id: 100%|█| 1170251/1170251 [00:00<00:00, 1288649.\n",
      "\n",
      " >> MERGING VIDEOS...\n",
      "#WARNING: Problem in Merging: KeyError('videos')\n",
      "\n",
      " >> MERGING IMAGES...\n",
      "update key: images: 100%|██████████| 123287/123287 [00:00<00:00, 1567102.73it/s]\n",
      "update annotations : image_id: 100%|█| 1170251/1170251 [28:19<00:00, 688.72it/s]\n",
      "\n",
      " >> MERGING TRACKS...\n",
      "#WARNING: Problem in Merging: KeyError('tracks')\n",
      "\n",
      " >> MERGING SEGMENT_INFO...\n",
      "#WARNING: Problem in Merging: KeyError('segment_info')\n",
      "\n",
      " >> MERGING ANNOTATIONS...\n",
      "update key: annotations: 100%|████| 1170251/1170251 [00:01<00:00, 893092.23it/s]\n",
      "\n",
      " >> SAVING...\n",
      "JSON SAVED : /mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json\n",
      "\n",
      ">>Namespace(dataset_id=2, dir_key=['file_name', 'video'], initjson=False, mergefile='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/fulltao.json', molafile='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json', root_dir=None)\n",
      "get keys videos: 100%|██████████████████| 1488/1488 [00:00<00:00, 624299.73it/s]\n",
      "get keys annotations: 100%|█████████| 167751/167751 [00:00<00:00, 627389.67it/s]\n",
      "get keys tracks: 100%|██████████████████| 8132/8132 [00:00<00:00, 843424.34it/s]\n",
      "get keys images: 100%|████████████████| 54649/54649 [00:00<00:00, 689759.05it/s]\n",
      "get keys info: 100%|██████████████████████████| 6/6 [00:00<00:00, 344737.32it/s]\n",
      "get keys categories: 100%|██████████████| 1230/1230 [00:00<00:00, 629606.29it/s]\n",
      "get keys licenses: 100%|███████████████████████| 1/1 [00:00<00:00, 77672.30it/s]\n",
      "\n",
      " >> MERGING LICENSES...\n",
      "update key: licenses: 100%|████████████████████| 1/1 [00:00<00:00, 76260.07it/s]\n",
      "update images : license: 100%|███████| 54649/54649 [00:00<00:00, 3461879.74it/s]\n",
      "\n",
      " >> MERGING CATEGORIES...\n",
      "update key: categories: 100%|██████████| 1230/1230 [00:00<00:00, 1430272.78it/s]\n",
      "update annotations : category_id: 100%|█| 167751/167751 [00:01<00:00, 93666.96it\n",
      "update tracks : category_id: 100%|██████| 8132/8132 [00:00<00:00, 102612.79it/s]\n",
      "\n",
      " >> MERGING VIDEOS...\n",
      "update key: videos: 100%|██████████████| 1488/1488 [00:00<00:00, 1202065.55it/s]\n",
      "update annotations : video_id: 100%|█| 167751/167751 [00:02<00:00, 79207.06it/s]\n",
      "update tracks : video_id: 100%|██████████| 8132/8132 [00:00<00:00, 83032.48it/s]\n",
      "update images : video_id: 100%|████████| 54649/54649 [00:00<00:00, 80737.74it/s]\n",
      "\n",
      " >> MERGING IMAGES...\n",
      "update key: images: 100%|████████████| 54649/54649 [00:00<00:00, 1590078.04it/s]\n",
      "update annotations : image_id: 100%|██| 167751/167751 [01:19<00:00, 2099.21it/s]\n",
      "\n",
      " >> MERGING TRACKS...\n",
      "update key: tracks: 100%|██████████████| 8132/8132 [00:00<00:00, 1488590.76it/s]\n",
      "update annotations : track_id: 100%|█| 167751/167751 [00:10<00:00, 15505.62it/s]\n",
      "\n",
      " >> MERGING SEGMENT_INFO...\n",
      "#WARNING: Problem in Merging: KeyError('segment_info')\n",
      "\n",
      " >> MERGING ANNOTATIONS...\n",
      "update key: annotations: 100%|█████| 167751/167751 [00:00<00:00, 1048854.23it/s]\n",
      "\n",
      " >> SAVING...\n",
      "JSON SAVED : /mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json\n",
      "\n",
      ">>Namespace(dataset_id=3, dir_key=['file_name', 'video'], initjson=False, mergefile='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/LBO.json', molafile='/mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json', root_dir=None)\n",
      "get keys info: 100%|██████████████████████████| 6/6 [00:00<00:00, 235194.62it/s]\n",
      "get keys licenses: 100%|███████████████████████| 1/1 [00:00<00:00, 54471.48it/s]\n",
      "get keys categories: 100%|████████████████| 970/970 [00:00<00:00, 596725.56it/s]\n",
      "get keys videos: 0it [00:00, ?it/s]\n",
      "get keys images: 100%|████████████████| 17007/17007 [00:00<00:00, 479869.01it/s]\n",
      "get keys tracks: 0it [00:00, ?it/s]\n",
      "get keys segment_info: 0it [00:00, ?it/s]\n",
      "get keys annotations: 100%|███████████| 26389/26389 [00:00<00:00, 666503.01it/s]\n",
      "get keys video_annotations: 0it [00:00, ?it/s]\n",
      "get keys datasets: 100%|███████████████████████| 1/1 [00:00<00:00, 63550.06it/s]\n",
      "\n",
      " >> MERGING LICENSES...\n",
      "#WARNING: Problem in Merging: TypeError('string indices must be integers')\n",
      "\n",
      " >> MERGING CATEGORIES...\n",
      "update key: categories: 100%|████████████| 970/970 [00:00<00:00, 1536433.11it/s]\n",
      "update annotations : category_id: 100%|█| 26389/26389 [00:00<00:00, 414299.73it/\n",
      "\n",
      " >> MERGING VIDEOS...\n",
      "update key: videos: 0it [00:00, ?it/s]\n",
      "\n",
      " >> MERGING IMAGES...\n",
      "update key: images: 100%|████████████| 17007/17007 [00:00<00:00, 1294812.73it/s]\n",
      "update annotations : image_id: 100%|████| 26389/26389 [00:03<00:00, 7355.97it/s]\n",
      "\n",
      " >> MERGING TRACKS...\n",
      "update key: tracks: 0it [00:00, ?it/s]\n",
      "\n",
      " >> MERGING SEGMENT_INFO...\n",
      "update key: segment_info: 0it [00:00, ?it/s]\n",
      "\n",
      " >> MERGING ANNOTATIONS...\n",
      "update key: annotations: 100%|███████| 26389/26389 [00:00<00:00, 1291100.79it/s]\n",
      "\n",
      " >> SAVING...\n",
      "JSON SAVED : /mnt/Data/Work/EASYRIDE/P19/NC/yolov5/JSONS/annotations/cocotaolbo.json\n"
     ]
    }
   ],
   "source": [
    "#WARNING if memory error go to a terminal ipython shell and paste this comands\n",
    "!python annotate_v5.py --molafile $molafile --mergefile $mergecoco --dataset_id 1\n",
    "!python annotate_v5.py --molafile $molafile --mergefile $mergetao --dataset_id 2 \n",
    "!python annotate_v5.py --molafile $molafile --mergefile $mergelbo --dataset_id 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. TEST MERGED JSON ANNOTATIONS DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "molajson = json.load(open(molafile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info 5\n",
      "licenses 9\n",
      "categories 2280\n",
      "videos 1488\n",
      "images 187045\n",
      "tracks 8132\n",
      "segment_info 0\n",
      "annotations 1352571\n",
      "datasets 3\n"
     ]
    }
   ],
   "source": [
    "for k in molajson:\n",
    "    print(k, len(molajson[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1352571/1352571 [00:00<00:00, 2187200.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352571\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# annotations category_id\n",
    "ann_ids=[]\n",
    "for an in tqdm(molajson['annotations']):\n",
    "    ann_ids.append(an['id'])\n",
    "print(len(ann_ids))\n",
    "\n",
    "#TEST duplicates v3 -faster\n",
    "u, c = np.unique(np.array(ann_ids), return_counts=True)\n",
    "duplicates_l= u[c > 1].tolist()\n",
    "print(len(duplicates_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNOTATIONS FORMAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOLA\n",
    "Format in annotate_v5.init_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "        \"info\": None,\n",
    "        \"licenses\": [],\n",
    "        \"categories\": [],\n",
    "        \"videos\": [],\n",
    "        \"images\": [],\n",
    "        \"tracks\": [],\n",
    "        \"segment_info\": [],\n",
    "        \"annotations\": [],\n",
    "        \"datasets\": [{'name': 'COCO', 'id': 1}, {'name': 'TAO', 'id': 2}]\n",
    "    }\n",
    "    \n",
    "output['info'] = {\n",
    "        \"description\": \"Mixed Dataset\",\n",
    "        \"url\": \"\",\n",
    "        \"version\": \"1\",\n",
    "        \"year\": 2020,\n",
    "        \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation file format: https://cocodataset.org/#format-data ; https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"info\": {info},\n",
    "    \"licenses\": [license],\n",
    "    \"images\": [image],\n",
    "    \"annotations\": [annotation],\n",
    "    \"categories\": [category], <-- Not in Captions annotations\n",
    "    \"segment_info\": [segment] <-- Only in Panoptic annotations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info{\n",
    "    \"year\": int, \n",
    "    \"version\": str, \n",
    "    \"description\": str, \n",
    "    \"contributor\": str, \n",
    "    \"url\": str, \n",
    "    \"date_created\": datetime,\n",
    "}\n",
    "license{\n",
    "    \"id\": int, \n",
    "    \"name\": str, \n",
    "    \"url\": str,\n",
    "}\n",
    "image{\n",
    "    \"id\": int, \n",
    "    \"width\": int, \n",
    "    \"height\": int, \n",
    "    \"file_name\": str, \n",
    "    \"license\": int, \"flickr_url\": str, \n",
    "    \"coco_url\": str, \"date_captured\": datetime,\n",
    "}\n",
    "annotation{\n",
    "    \"id\": int, \n",
    "    \"image_id\": int, \n",
    "    \"category_id\": int, \n",
    "    \"segmentation\": RLE or [polygon], \n",
    "    \"area\": float, \n",
    "    \"bbox\": [x,y,width,height], \n",
    "    \"iscrowd\": 0 or 1,\n",
    "}\n",
    "\n",
    "category{\n",
    "    \"id\": int, \n",
    "    \"name\": str, \n",
    "    \"supercategory\": str,\n",
    "}\n",
    "segment{\n",
    "    \"id\": int, \n",
    "    \"category_id\": int, \n",
    "    \"area\": int, \n",
    "    \"bbox\": [x,y,width,height], \n",
    "    \"iscrowd\": 0 or 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation file format: https://github.com/TAO-Dataset/tao/blob/master/tao/toolkit/tao/tao.py\n",
    "\n",
    "\n",
    "#NOTE: https://github.com/TAO-Dataset/tao/blob/master/docs/faqs.md . Why does the training set only contain 216 LVIS categories?\n",
    "- TAO contains a total of 482 LVIS categories. However, not all categories are present in the train, val, and test sets. Instead, we encourage researchers to train detectors on the LVIS v0.5 dataset, which contains a superset of the 482 categories, and trackers on existing single-object tracking datasets. TAO is primarily a benchmark dataset, but we provide a small set of training videos for tuning trackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"info\" : info,\n",
    "    \"images\" : [image],\n",
    "    \"videos\": [video],\n",
    "    \"tracks\": [track],\n",
    "    \"annotations\" : [annotation],\n",
    "    \"categories\": [category],\n",
    "    \"licenses\" : [license],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info: \"like MS COCO\"\n",
    "\n",
    "license: {\n",
    "    \"id\" : int,\n",
    "    \"name\" : str,\n",
    "    \"url\" : str,\n",
    "}\n",
    "category: {\n",
    "    \"id\": int,\n",
    "    \"name\": str,\n",
    "    \"synset\": str,  # For non-LVIS objects, this is \"unknown\"\n",
    "    ... [other fields copied from LVIS v0.5 and unused]\n",
    "}\n",
    "\n",
    "video: {\n",
    "    \"id\": int,\n",
    "    \"name\": str,\n",
    "    \"width\" : int,\n",
    "    \"height\" : int,\n",
    "    \"neg_category_ids\": [int],\n",
    "    \"not_exhaustive_category_ids\": [int],\n",
    "    \"metadata\": dict,  # Metadata about the video\n",
    "}\n",
    "image: {\n",
    "    \"id\" : int,\n",
    "    \"video_id\": int,\n",
    "    \"file_name\" : str,\n",
    "    \"license\" : int,\n",
    "    # Redundant fields for COCO-compatibility\n",
    "    \"width\": int,\n",
    "    \"height\": int,\n",
    "    \"frame_index\": int\n",
    "}    \n",
    "track: {\n",
    "    \"id\": int,\n",
    "    \"category_id\": int,\n",
    "    \"video_id\": int\n",
    "}\n",
    "annotation: {\n",
    "    \"image_id\": int,\n",
    "    \"track_id\": int,\n",
    "    \"bbox\": [x,y,width,height],\n",
    "    \"area\": float,\n",
    "    # Redundant field for compatibility with COCO scripts\n",
    "    \"category_id\": int\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
