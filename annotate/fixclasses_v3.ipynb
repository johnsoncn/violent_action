{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Classes=Categories\n",
    "version: 3\n",
    "\n",
    "info: \n",
    "- fix classes:\n",
    "    - find and fix duplicates:\n",
    "        - equal names\n",
    "        - similar names\n",
    "\n",
    "author: nuno costa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: Windows-10-10.0.20241-SP0\n",
      "root dir: D:/external_datasets/\n"
     ]
    }
   ],
   "source": [
    "from annotate_v5 import *\n",
    "import platform \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "import copy\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "\n",
    "#Define root dir dependent on OS\n",
    "rdir='D:/external_datasets/' \n",
    "if str(platform.platform()).find('linux')>-1: rdir='/mnt/d/external_datasets/' \n",
    "print('OS: {}'.format(platform.platform()))\n",
    "print('root dir: {}'.format(rdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "molajson =  json.load(open(rdir+'mola.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info 5\n",
      "licenses 9\n",
      "categories 1310\n",
      "videos 1488\n",
      "images 177936\n",
      "tracks 8132\n",
      "segment_info 0\n",
      "annotations 1338002\n",
      "datasets 2\n"
     ]
    }
   ],
   "source": [
    "for k in molajson:\n",
    "    print(k, len(molajson[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import ids\n",
    "#### #NOTE: work with ids and index so you can use numpy for faster operations\n",
    "#### #WARNING don't use try: except: pass when importing - go back to mergedatasets and find the BUG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COCO', 'TAO'] [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# datasets name and id\n",
    "dset_l=[]\n",
    "dset_l_id=[]\n",
    "for d in molajson['datasets']:\n",
    "    dset_l.append(d['name'])\n",
    "    dset_l_id.append(d['id'])\n",
    "print(dset_l, dset_l_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories name and id\n",
    "cat_l=[]\n",
    "cat_l_id=[]\n",
    "cat_l_dset=[]\n",
    "for c in molajson['categories']:\n",
    "    cat_l.append(c['name'])\n",
    "    cat_l_id.append(c['id'])\n",
    "    cat_l_dset.append(dset_l[c['dataset']-1]) # dset_l index is same as id-1\n",
    "#print(cat_l_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images filepath and id\n",
    "img_l=[]\n",
    "img_l_id=[]\n",
    "for c in molajson['images']:\n",
    "    img_l.append(c['file_name'])\n",
    "    img_l_id.append(c['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1338002/1338002 [00:02<00:00, 454771.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# annotations category_id, image_id, bbox, and dataset\n",
    "ann_catid=[]\n",
    "ann_imgid=[]\n",
    "ann_bbox=[]\n",
    "ann_dset=[]\n",
    "for an in tqdm(molajson['annotations']):\n",
    "    ann_catid.append(an['category_id'])\n",
    "    ann_imgid.append(an['image_id'])\n",
    "    ann_bbox.append(an['bbox'])\n",
    "    ann_dset.append(an['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find duplicates cat_ids\n",
    "Duplicates example\n",
    "categories= [{name:cow, id:1, dataset:1},...,{name:cow, id:200, dataset:2},...,{name:cow, id:101, dataset:3}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 1689.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['boat', 'sheep', 'frisbee', 'carrot', 'cup', 'vase', 'toilet', 'scissors', 'fork', 'toothbrush', 'apple', 'knife', 'dog', 'bed', 'sink', 'skateboard', 'motorcycle', 'broccoli', 'giraffe', 'bowl', 'truck', 'bench', 'bird', 'kite', 'suitcase', 'sandwich', 'backpack', 'spoon', 'chair', 'bear', 'cat', 'umbrella', 'cow', 'cake', 'refrigerator', 'bicycle', 'horse', 'book', 'toaster', 'pizza', 'bottle', 'elephant', 'banana', 'airplane', 'surfboard', 'clock', 'handbag', 'zebra', 'snowboard']\n",
      "[[9, 198], [19, 1041], [30, 560], [52, 301], [42, 427], [76, 1242], [62, 1197], [77, 1017], [43, 555], [80, 1202], [48, 93], [44, 705], [17, 462], [60, 159], [72, 1059], [37, 1060], [4, 794], [51, 234], [24, 582], [46, 219], [8, 1224], [14, 171], [15, 179], [34, 701], [29, 116], [49, 1006], [25, 114], [45, 1098], [57, 317], [22, 158], [16, 309], [26, 1235], [20, 161], [56, 265], [73, 508], [2, 175], [18, 659], [74, 206], [71, 1195], [54, 909], [40, 213], [21, 509], [47, 125], [5, 84], [38, 1137], [75, 356], [27, 115], [23, 1309], [32, 1073]]\n",
      "[['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO'], ['COCO', 'TAO']]\n",
      "49\n",
      "49\n",
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#duplicates #TODO: SORT alphabetically\n",
    "duplicates_l=[]\n",
    "duplicates_l_catid=[]\n",
    "duplicates_l_catdset=[]\n",
    "duplicate_method=\"equal_names\"\n",
    "if duplicate_method==\"equal_names\": #FIX REPETITIONS\n",
    "    duplicates_l=list(set([x for x in cat_l if cat_l.count(x) > 1])) # duplicates l \n",
    "    for duplicate in tqdm(duplicates_l):\n",
    "        idx_mask = [name == duplicate for name in cat_l] #mask of index of duplicate\n",
    "        catids = np.array(cat_l_id)[idx_mask] #duplicate catids\n",
    "        catdsets = np.array(cat_l_dset)[idx_mask] #duplicate catdset\n",
    "        duplicates_l_catid.append(catids.tolist())\n",
    "        duplicates_l_catdset.append(catdsets.tolist())\n",
    "if duplicate_method==\"similar_names\": #FIX SIMILAR NAMES\n",
    "    #WARNING: fix equal_names first - if not, it will not be fixed in this case\n",
    "    import difflib\n",
    "    for cat in cat_l: \n",
    "        match_l=[]\n",
    "        match_l_temp=[c for c in cat_l if (c).find(cat)>-1] #only substring inside string - catagory with big name is not found\n",
    "        if not len(match_l_temp)>1: continue #more than one\n",
    "        #refine search #ADD or REMOVE refine options\n",
    "        match_l.append(cat) #make category to be the first item\n",
    "        match_l.extend([c for c in match_l_temp if ((c+' ').find(cat)>-1 or (c+' ').find(cat)>-1 or (' '+c).find(cat)>-1 or (c+'_').find(cat)>-1 or ('_'+c).find(cat)>-1 )]) #mantain optins: c+'_';'_'+c \n",
    "        match_l.extend(difflib.get_close_matches(cat, match_l_temp, n=5, cutoff=0.9)) #get similar words\n",
    "        #remove equal names\n",
    "        match_l=list(dict.fromkeys(match_l))#match_l=list(set(match_l))\n",
    "        if not len(match_l)>1: continue #more than one\n",
    "        #add to duplicates\n",
    "        duplicates_l.append(match_l)\n",
    "    for duplicates in tqdm(duplicates_l):\n",
    "        catids=[]\n",
    "        catdsets=[]\n",
    "        for duplicate in duplicates:\n",
    "            idx=cat_l.index(duplicate) #first duplicate ()\n",
    "            catids.append(cat_l_id[idx]) #duplicate catids\n",
    "            catdsets.append(cat_l_dset[idx]) #duplicate cat dsets\n",
    "        duplicates_l_catid.append(catids)\n",
    "        duplicates_l_catdset.append(catdsets)\n",
    "if duplicate_method==\"all_names\": #Do for all category names, even with equal \n",
    "    duplicates_l=cat_l #NORMALIZE: same list format ?? not needed\n",
    "    duplicates_l_catid=[[id] for id in cat_l_id]\n",
    "    duplicates_l_catdset=[[dset] for dset in cat_l_dset]\n",
    "    \n",
    "\n",
    "print(duplicates_l)\n",
    "print(duplicates_l_catid)\n",
    "print(duplicates_l_catdset)\n",
    "print(len(duplicates_l))\n",
    "print(len(duplicates_l_catid))\n",
    "print(len(duplicates_l_catdset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 84.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# get annotations duplicates\n",
    "ann_catid_np=np.array(ann_catid)\n",
    "ann_imgid_np=np.array(ann_imgid)\n",
    "ann_bbox_np=np.array(ann_bbox)\n",
    "ann_dset_np=np.array(ann_dset)\n",
    "duplicates_l_imgid=[]\n",
    "duplicates_l_bbox=[]\n",
    "duplicates_l_dset=[]\n",
    "for catids in tqdm(duplicates_l_catid):\n",
    "    l_imgid=[]\n",
    "    l_bbox=[]\n",
    "    l_dset=[]\n",
    "    for catid in catids:\n",
    "        ann_idx = np.where(ann_catid_np==catid)[0].tolist() #annotation index of ids\n",
    "        l_imgid.append(ann_imgid_np[ann_idx].tolist())\n",
    "        l_bbox.append(ann_bbox_np[ann_idx].tolist())\n",
    "        l_dset.append(ann_dset_np[ann_idx].tolist())\n",
    "    duplicates_l_imgid.append(l_imgid)\n",
    "    duplicates_l_bbox.append(l_bbox)\n",
    "    duplicates_l_dset.append(l_dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classes|categories to fix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classtofix_l=[]\n",
    "classtofix_l_catid=[]\n",
    "method=\"all\"\n",
    "if method==\"all\": #fix all duplicates\n",
    "    classtofix_l=duplicates_l\n",
    "    classtofix_l_catid=duplicates_l_catid\n",
    "if method==\"view_images\": # view images and select class to fix (INCEPTION =>>>Pufff!!!!)\n",
    "    img_l_id_np=np.array(img_l_id)      \n",
    "    for i, duplicate in enumerate(tqdm(duplicates_l)): #run for each duplicate\n",
    "        print(duplicate+'...') #class\n",
    "        display_imgs(rdir, i, dset_l, duplicates_l_catid, duplicates_l_dset, duplicates_l_imgid, img_l, img_l_id, imgidx=0)\n",
    "        classtofix_l, classtofix_l_catid=save_classtofix(i, classtofix_l, classtofix_l_catid, duplicates_l, duplicates_l_catid, duplicates_l_dset, duplicates_l_imgid, img_l, img_l_id, imgidx=0)\n",
    "if method==\"save_images\": # save images to folder for manual check\n",
    "    datadir=\"duplicates\"\n",
    "    folder=duplicate_method+'/'\n",
    "    showimage=False #show images\n",
    "    startidx=0 # start index of image to save from each dataset\n",
    "    imgnr=10 # total number of images to save from each dataset\n",
    "    imgstep='random' # step between images: int | 'random' - int steps between images; 'rand' gets random list\n",
    "    path=os.path.join(rdir,datadir,folder)\n",
    "    assure_path_exists(path)\n",
    "    #TODO\n",
    "    #save excel # TODO Send duplicates_l, duplicates_L_catid, duplicates_l_catdset: user should make column  classtofix_l and classtofix_L_catid - user should create  \n",
    "    df=pd.DataFrame({'duplicates_l': duplicates_l,'duplicates_l_catid': duplicates_l_catid, 'duplicates_l_catdset': duplicates_l_catdset, 'classtofix_l': np.nan, 'classtofix_l_catid':np.nan, 'rules':np.nan })\n",
    "    df.loc[0, 'rules']=\"To fix classes: 1) You need to fill the column classtofix_l and/or classtofix_l_catid with the information from the respective duplicate columns; 2) When copy/pasting or changing, make sure the same structure maintains:  ['car', 'carrot'], [3, 52], beware of the spaces ['car', '  and always maintain the first class in the list;  3) You have 3 possibilities of filling the columns : 1-the 2 columns empty, meaning the row will not be used for classtofix; 2-only one column empty, e.g. fill the classtotix_l row with the class labels from duplicates_l, then during the importing the classtofix_l_catid is filled, and vice-versa; 3-If you want to change the name of the first class in the list,e.g ['car', 'carrot'] for ['automobile', 'carrot'] you need to provide the ids to classtofix_l_catid.\"\n",
    "    excelpath=path+duplicate_method+\"_classtofix_report.xlsx\"\n",
    "    df['annotations_missing'] = np.empty((len(df), 0)).tolist()\n",
    "    df['images_missing'] = np.empty((len(df), 0)).tolist()\n",
    "    #save image for each duplicate\n",
    "    for i, duplicate in enumerate(tqdm(duplicates_l)): #run for each duplicate category\n",
    "        minclass=duplicate\n",
    "        if isinstance(minclass, list): minclass=minclass[0]\n",
    "        print('\\n>> '+minclass+'...') #class\n",
    "        classpath=os.path.join(path, minclass) # folder for images of this class\n",
    "        classpath=parse_path(classpath)+'/' #make it a folder\n",
    "        assure_path_exists(classpath)\n",
    "        df=save_imgs(df, rdir, classpath, i, dset_l, duplicates_l, duplicates_l_catid, duplicates_l_bbox, duplicates_l_dset,\n",
    "              duplicates_l_imgid, img_l, img_l_id, startidx=startidx, imgnr=imgnr, imgstep=imgstep, showimage=showimage)\n",
    "    df.to_excel(excelpath, index=False)   \n",
    "if method==\"view_duplicate\":\n",
    "    i=-1 #duplicate_l index : last category\n",
    "    ii=1 #dset_l index : TAO\n",
    "    step=10 #step images\n",
    "    dpi=80\n",
    "    dataset=dset_l[ii]\n",
    "    category=duplicates_l[i]\n",
    "    view_duplicate(rdir, i, ii, dataset, category, duplicates_l_imgid, duplicates_l_bbox, img_l, img_l_id, step=step,\n",
    "                   dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: CHECK EXCEL FIRST\n",
    "if method==\"save_images\": # reading classtofix from excel\n",
    "    classtofix_l=[]\n",
    "    classtofix_l_catid=[]\n",
    "    df=pd.read_excel(excelpath)\n",
    "    classtofix_df=df.loc[:,'classtofix_l']\n",
    "    classtofix_df_catid=df.loc[:,'classtofix_l_catid']\n",
    "    display(df)\n",
    "    # PARSE COLUMNS TO FIX\n",
    "    classtofix_l=classtofix_df.tolist()\n",
    "    classtofix_l_catid=classtofix_df_catid.tolist()\n",
    "    print(classtofix_l_catid)\n",
    "    #convert strings to lists\n",
    "    for icl, cl in enumerate(classtofix_l): \n",
    "        if isinstance(classtofix_l[icl], str): classtofix_l[icl]=convert_unicode(classtofix_l[icl], method='liststr')\n",
    "        if isinstance(classtofix_l_catid[icl], str): classtofix_l_catid[icl]=convert_unicode(classtofix_l_catid[icl], method='listnum')\n",
    "    print(classtofix_l_catid)\n",
    "    #parse the columns based on the rules    \n",
    "    for ic, classes in enumerate(classtofix_df):\n",
    "        #1. Two columns empty - do nothing, maintain\n",
    "        if  pd.isnull(classtofix_df.iloc[ic]) and pd.isnull(classtofix_df_catid.iloc[ic]): continue\n",
    "        #2. if only classtofix_l_catid empty - get \n",
    "        if not pd.isnull(classtofix_df.iloc[ic]) and pd.isnull(classtofix_df_catid.iloc[ic]):\n",
    "            classes=convert_unicode(classes, method='liststr')\n",
    "            cids=[]\n",
    "            for c in classes:\n",
    "                cidx=duplicates_l[ic].index(c)\n",
    "                cid=duplicates_l_catid[ic][cidx]\n",
    "                cids.append(cid)\n",
    "            classtofix_l_catid[ic]=cids\n",
    "            print(classtofix_l_catid)\n",
    "        #2. if only classtofix_l empty - get\n",
    "        if pd.isnull(classtofix_df.iloc[ic]) and not pd.isnull(classtofix_df_catid.iloc[ic]): \n",
    "            classes=[]\n",
    "            cids=classtofix_l_catid[ic]\n",
    "            for c in cids:\n",
    "                cidx=duplicates_l_catid[ic].index(c)\n",
    "                clas=duplicates_l[ic][cidx]\n",
    "                classes.append(clas)\n",
    "            classtofix_l[ic]=classes\n",
    "        #3. if both columns not empty - do nothing\n",
    "        if not pd.isnull(classtofix_df.iloc[ic]) and not pd.isnull(classtofix_df_catid.iloc[ic]): continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Make sure everything is correct: 1.Drop NaN if exist, but make sure the index is the same for the two! \n",
      "\n",
      "['boat', 'sheep', 'frisbee', 'carrot', 'cup', 'vase', 'toilet', 'scissors', 'fork', 'toothbrush', 'apple', 'knife', 'dog', 'bed', 'sink', 'skateboard', 'motorcycle', 'broccoli', 'giraffe', 'bowl', 'truck', 'bench', 'bird', 'kite', 'suitcase', 'sandwich', 'backpack', 'spoon', 'chair', 'bear', 'cat', 'umbrella', 'cow', 'cake', 'refrigerator', 'bicycle', 'horse', 'book', 'toaster', 'pizza', 'bottle', 'elephant', 'banana', 'airplane', 'surfboard', 'clock', 'handbag', 'zebra', 'snowboard']\n",
      "[[9, 198], [19, 1041], [30, 560], [52, 301], [42, 427], [76, 1242], [62, 1197], [77, 1017], [43, 555], [80, 1202], [48, 93], [44, 705], [17, 462], [60, 159], [72, 1059], [37, 1060], [4, 794], [51, 234], [24, 582], [46, 219], [8, 1224], [14, 171], [15, 179], [34, 701], [29, 116], [49, 1006], [25, 114], [45, 1098], [57, 317], [22, 158], [16, 309], [26, 1235], [20, 161], [56, 265], [73, 508], [2, 175], [18, 659], [74, 206], [71, 1195], [54, 909], [40, 213], [21, 509], [47, 125], [5, 84], [38, 1137], [75, 356], [27, 115], [23, 1309], [32, 1073]]\n"
     ]
    }
   ],
   "source": [
    "print('>> Make sure everything is correct: 1.Drop NaN if exist, but make sure the index is the same for the two! \\n')\n",
    "fixempty=True\n",
    "if fixempty:\n",
    "    classtofix_l=[x for x in classtofix_l if str(x) != 'nan' and str(x) !='[]']\n",
    "    classtofix_l_catid=[x for x in classtofix_l_catid if str(x) != 'nan' and str(x) !='[]']\n",
    "print(classtofix_l)\n",
    "print(classtofix_l_catid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fix classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow # newjson=copy.deepcopy(molajson) #do deepcopy to compare\n",
    "# fast\n",
    "newjson={'categories':[],'annotations':[] }\n",
    "newjson['categories']=copy.copy(molajson['categories'])\n",
    "newjson['annotations']=copy.copy(molajson['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "classtofix_l_catidx=[[cat_l_id.index(id) for id in id_l] for id_l in classtofix_l_catid]\n",
    "#print(classtofix_l_catidx) # they should be less one, becacuse it is ordered\n",
    "print(len(classtofix_l_catidx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change molajson['categories']: [{name: , id: }]  \n",
    "\n",
    "- Use min cat id and remove the other categories (!!!Without ordering again the category id!!!)\n",
    "\n",
    "- #WARNING  the fixed category will only have the key {dataset: } with the first corresponding dataset. \n",
    "   - #SOLUTION change {dataset: int } to {dataset: list } in init_json, than in all the functions that deal with the int make the appropriate changes. Then, you can change molajson['categories']: [dataset: }]  using duplicates_l_catdset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "removeidx_l=[]\n",
    "minidx=0 # get first cat: minimum in equal names and the category used to search in similar_names\n",
    "for i,id_l in enumerate(tqdm(classtofix_l_catid)): #for each classtofix\n",
    "    minid=id_l[minidx] # #category id \n",
    "    catidx=classtofix_l_catidx[i][minidx]# get cat index of min catid\n",
    "    if isinstance(classtofix_l[i], list): newjson['categories'][firstcatidx]['name']=classtomix_l[i][minidx] #change name of first id \n",
    "    else: newjson['categories'][catidx]['name']=classtofix_l[i] #change name of min id (if changed)\n",
    "    assert newjson['categories'][catidx]['id']==id_l[minidx] #assert id - it should be the same\n",
    "    otheridx_l=copy.copy(classtofix_l_catidx[i]) #the idx to remove\n",
    "    otheridx_l.remove(catidx)\n",
    "    removeidx_l.extend(otheridx_l) #remove index\n",
    "removeidx_l=list(dict.fromkeys(removeidx_l)) # no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE - Newjson will be changed\n",
    "removeitem_l=[newjson['categories'][removeidx] for removeidx in removeidx_l] #items to remove\n",
    "for removeitem in removeitem_l: newjson['categories'].remove(removeitem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supercategory': 'outdoor', 'id': 14, 'name': 'bench', 'dataset': 1}\n",
      "{'supercategory': 'outdoor', 'id': 14, 'name': 'bench', 'dataset': 1}\n",
      "{'frequency': 'f', 'id': 171, 'synset': 'bench.n.01', 'image_count': 93, 'instance_count': 160, 'synonyms': ['bench'], 'def': 'a long seat for more than one person', 'name': 'bench', 'dataset': 2}\n",
      "{'frequency': 'c', 'id': 183, 'synset': 'birdhouse.n.01', 'image_count': 0, 'instance_count': 0, 'synonyms': ['birdhouse'], 'def': 'a shelter for birds', 'name': 'birdhouse', 'dataset': 2}\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "print(molajson['categories'][13])\n",
    "print(newjson['categories'][13])\n",
    "print(molajson['categories'][170])\n",
    "print(newjson['categories'][170])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change molajson['annotations']: [{category_id: , }] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "ann_catid_np=np.array(ann_catid)\n",
    "classtofix_l_ann_catidx=[[np.where(ann_catid_np==id)[0].tolist()  for id in id_l] for id_l in classtofix_l_catid]\n",
    "print(len(classtofix_l_ann_catidx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,id_l in enumerate(classtofix_l_catid): #for each classtofix\n",
    "    minidx=0 # get first catid min\n",
    "    minid=id_l[minidx]\n",
    "    ann_catidx= classtofix_l_ann_catidx[i][minidx]# get annotation cat index of min catid\n",
    "    ann_otheridx_l=copy.copy(classtofix_l_ann_catidx[i])\n",
    "    ann_otheridx_l.remove(ann_catidx) #the idx to change\n",
    "    for ann_otheridx in ann_otheridx_l[0]: newjson['annotations'][ann_otheridx]['category_id']=minid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'segmentation': [[20.95, 105.11, 20.95, 87.58, 8.43, 80.69, 8.43, 75.06, 38.48, 74.43, 40.35, 66.92, 32.22, 63.79, 39.1, 35.62, 135.51, 37.5, 130.5, 65.04, 128.0, 70.05, 124.87, 108.86, 119.86, 102.6, 119.23, 86.95, 111.72, 86.95, 106.08, 107.61, 104.83, 107.61, 98.57, 90.71, 77.91, 86.33, 64.77, 86.33, 55.38, 90.08, 45.99, 100.73, 45.36, 109.49, 41.61, 110.12, 40.35, 97.6, 35.97, 89.46, 29.71, 86.95, 24.7, 93.84, 24.7, 105.73]], 'area': 5697.957150000001, 'iscrowd': 0, 'image_id': 26929, 'bbox': [8.43, 35.62, 127.08, 74.5], 'category_id': 14, 'id': 8876, 'dataset': 1}\n",
      "{'segmentation': [[20.95, 105.11, 20.95, 87.58, 8.43, 80.69, 8.43, 75.06, 38.48, 74.43, 40.35, 66.92, 32.22, 63.79, 39.1, 35.62, 135.51, 37.5, 130.5, 65.04, 128.0, 70.05, 124.87, 108.86, 119.86, 102.6, 119.23, 86.95, 111.72, 86.95, 106.08, 107.61, 104.83, 107.61, 98.57, 90.71, 77.91, 86.33, 64.77, 86.33, 55.38, 90.08, 45.99, 100.73, 45.36, 109.49, 41.61, 110.12, 40.35, 97.6, 35.97, 89.46, 29.71, 86.95, 24.7, 93.84, 24.7, 105.73]], 'area': 5697.957150000001, 'iscrowd': 0, 'image_id': 26929, 'bbox': [8.43, 35.62, 127.08, 74.5], 'category_id': 14, 'id': 8876, 'dataset': 1}\n",
      "{'segmentation': [[419, 478, 911, 478, 911, 638, 419, 638]], 'bbox': [419, 478, 492, 160], 'area': 78720, 'iscrowd': 0, 'id': 1174301, 'image_id': 124678, 'category_id': 14, 'track_id': 188, '_scale_uuid': '5537baa6-978f-47ef-ab14-494f7aa3b412', 'scale_category': 'moving object', 'video_id': 37, 'dataset': 2}\n",
      "{'segmentation': [[419, 478, 911, 478, 911, 638, 419, 638]], 'bbox': [419, 478, 492, 160], 'area': 78720, 'iscrowd': 0, 'id': 1174301, 'image_id': 124678, 'category_id': 14, 'track_id': 188, '_scale_uuid': '5537baa6-978f-47ef-ab14-494f7aa3b412', 'scale_category': 'moving object', 'video_id': 37, 'dataset': 2}\n",
      "\n",
      ">> mlabjson \n",
      "\n",
      "info 5\n",
      "licenses 9\n",
      "categories 1310\n",
      "videos 1488\n",
      "images 177936\n",
      "tracks 8132\n",
      "segment_info 0\n",
      "annotations 1338002\n",
      "datasets 2\n",
      "\n",
      ">> newjson \n",
      "\n",
      "categories 1261\n",
      "annotations 1338002\n"
     ]
    }
   ],
   "source": [
    "#TEST\n",
    "print(molajson['annotations'][8875])\n",
    "print(newjson['annotations'][8875])\n",
    "print(molajson['annotations'][1174300])\n",
    "print(newjson['annotations'][1174300])\n",
    "\n",
    "print('\\n>> molajson \\n')\n",
    "for k in molajson:\n",
    "    print(k, len(molajson[k]))\n",
    "\n",
    "print('\\n>> newjson \\n')    \n",
    "for k in newjson:\n",
    "    print(k, len(newjson[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save fixed json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast\n",
    "molajson['categories']=copy.copy(newjson['categories'])\n",
    "molajson['annotations']=copy.copy(newjson['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "print('\\n >> SAVING...')\n",
    "jsonfile=rdir+'mola_fix_equal.json'\n",
    "with open(jsonfile, 'w') as f:\n",
    "    json.dump(molajson, f)\n",
    "print(\"JSON SAVED : {} \\n\".format(jsonfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
